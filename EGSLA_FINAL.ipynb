{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPAcdTxYzZeCBSdHNW8Q1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahulappu2004/Fake-Account-Detection-in-Twitter-X/blob/main/EGSLA_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn networkx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYPsxamlLmwe",
        "outputId": "4032ceba-f12a-4a21-f643-5417f8ce392e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "ZvJ9vGtXO1dN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/twitter_data.csv')"
      ],
      "metadata": {
        "id": "REjI2LrXO50P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Extract features and target\n",
        "X = df.drop('Fake Or Not Category', axis=1)\n",
        "y = df['Fake Or Not Category']"
      ],
      "metadata": {
        "id": "C6zvjWhIPQEy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "UY_GoiXjPY5b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labeled and unlabeled data\n",
        "np.random.seed(42)\n",
        "labeled_indices = np.random.choice(len(y), size=int(0.3 * len(y)), replace=False)\n",
        "unlabeled_indices = np.setdiff1d(np.arange(len(y)), labeled_indices)\n",
        "\n",
        "y_semi = -1 * np.ones_like(y)\n",
        "y_semi[labeled_indices] = y[labeled_indices]"
      ],
      "metadata": {
        "id": "NAl_DP6mPdjc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.1  # Lower gamma reduces similarity\n",
        "similarity_matrix = rbf_kernel(X_scaled, gamma=gamma)\n",
        "# Set a stricter similarity threshold\n",
        "G = nx.Graph()\n",
        "for i in range(len(X_scaled)):\n",
        "    for j in range(i + 1, len(X_scaled)):\n",
        "        if similarity_matrix[i, j] > 0.3:  # Increase threshold\n",
        "            G.add_edge(i, j, weight=similarity_matrix[i, j])"
      ],
      "metadata": {
        "id": "8sTbuGxcPpxa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Enhance the Graph (optional, add enhancements specific to EGSLA)\n",
        "# Example: Remove noisy edges (edges with very low weight)\n",
        "edges_to_remove = [(u, v) for u, v, w in G.edges(data=True) if w['weight'] < 0.4]\n",
        "G.remove_edges_from(edges_to_remove)\n",
        "# Step 3: Label Propagation\n",
        "# Initialize labels for propagation\n",
        "labels = np.copy(y_semi)"
      ],
      "metadata": {
        "id": "UbsO9voNP65U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform label propagation\n",
        "max_iterations = 10\n",
        "for iteration in range(max_iterations):\n",
        "    new_labels = np.copy(labels)\n",
        "    for node in G.nodes():\n",
        "        if node in labeled_indices:  # Skip labeled nodes\n",
        "            continue\n",
        "        # Aggregate labels from neighbors\n",
        "        neighbors = list(G.neighbors(node))\n",
        "        neighbor_labels = [labels[neighbor] for neighbor in neighbors if labels[neighbor] != -1]\n",
        "        if neighbor_labels:  # If neighbors have labels\n",
        "            new_labels[node] = max(set(neighbor_labels), key=neighbor_labels.count)\n",
        "    # Check for convergence\n",
        "    if np.array_equal(labels, new_labels):\n",
        "        break\n",
        "    labels = new_labels"
      ],
      "metadata": {
        "id": "HzolfjICQJEG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated dataset\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "num_features = 7\n",
        "X = np.random.rand(num_samples, num_features)  # Feature matrix\n",
        "y = np.random.randint(0, 2, size=num_samples)  # Binary labels\n",
        "\n"
      ],
      "metadata": {
        "id": "F4nZBkPkWX5o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Add noise to features\n",
        "noise = np.random.normal(0, 0.1, X_scaled.shape)  # Noise with mean=0, std=0.1\n",
        "X_scaled_noisy = X_scaled + noise\n"
      ],
      "metadata": {
        "id": "9bHAB4mQfWq7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split labeled and unlabeled indices\n",
        "labeled_ratio = 0.2  # Use 20% of the data as labeled\n",
        "labeled_indices = np.random.choice(len(y), size=int(labeled_ratio * len(y)), replace=False)\n",
        "unlabeled_indices = np.setdiff1d(np.arange(len(y)), labeled_indices)"
      ],
      "metadata": {
        "id": "6uncC1Z0fZgZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce label noise (20% mislabeled data)\n",
        "num_mislabeled = int(0.2 * len(labeled_indices))\n",
        "mislabeled_indices = np.random.choice(labeled_indices, size=num_mislabeled, replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy[mislabeled_indices] = 1 - y_noisy[mislabeled_indices]  # Flip labels"
      ],
      "metadata": {
        "id": "OCAJTobCfcS6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity matrix using RBF kernel with lower gamma\n",
        "gamma = 0.1\n",
        "similarity_matrix = rbf_kernel(X_scaled_noisy, gamma=gamma)\n",
        "\n",
        "# Construct a sparse graph (threshold-based)\n",
        "threshold = 0.4  # Higher threshold reduces graph connectivity\n",
        "G = nx.Graph()\n",
        "for i in range(len(X_scaled_noisy)):\n",
        "    for j in range(i + 1, len(X_scaled_noisy)):\n",
        "        if similarity_matrix[i, j] > threshold:\n",
        "            G.add_edge(i, j, weight=similarity_matrix[i, j])"
      ],
      "metadata": {
        "id": "2xFUkVV5fenC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize labels for semi-supervised learning\n",
        "y_semi = np.full(len(y), -1)  # -1 indicates unlabeled\n",
        "y_semi[labeled_indices] = y_noisy[labeled_indices]  # Use noisy labels for labeled data"
      ],
      "metadata": {
        "id": "kSHKEhurfmvT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semi-supervised label propagation (EGSLA-inspired)\n",
        "def label_propagation(graph, y_semi, max_iterations=50, alpha=0.9):\n",
        "    node_labels = np.zeros((len(graph.nodes), 2))  # One-hot encoded labels\n",
        "    for i in graph.nodes:\n",
        "        if y_semi[i] != -1:\n",
        "            node_labels[i, y_semi[i]] = 1  # Initialize labeled nodes\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        new_labels = np.zeros_like(node_labels)\n",
        "        for node in graph.nodes:\n",
        "            neighbors = list(graph.neighbors(node))\n",
        "            if y_semi[node] != -1:\n",
        "                new_labels[node] = node_labels[node]  # Preserve labeled data\n",
        "            else:\n",
        "                # Label propagation: weighted average of neighbors' labels\n",
        "                for neighbor in neighbors:\n",
        "                    weight = graph[node][neighbor]['weight']\n",
        "                    new_labels[node] += weight * node_labels[neighbor]\n",
        "                new_labels[node] = alpha * new_labels[node] + (1 - alpha) * node_labels[node]\n",
        "\n",
        "        # Normalize labels\n",
        "        new_labels = new_labels / (new_labels.sum(axis=1, keepdims=True) + 1e-6)\n",
        "        node_labels = new_labels\n",
        "\n",
        "    # Final predicted labels\n",
        "    return np.argmax(node_labels, axis=1)\n",
        "\n",
        "# Run label propagation\n",
        "predicted_labels = label_propagation(G, y_semi)"
      ],
      "metadata": {
        "id": "IYg28QedfuCr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance on labeled data\n",
        "true_labels = y[labeled_indices]\n",
        "predicted_on_labeled = predicted_labels[labeled_indices]\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_on_labeled)\n",
        "precision = precision_score(true_labels, predicted_on_labeled)\n",
        "recall = recall_score(true_labels, predicted_on_labeled)\n",
        "f1 = f1_score(true_labels, predicted_on_labeled)\n",
        "\n",
        "print(\"Performance Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkUsDtU_fx1n",
        "outputId": "91189a43-d4f8-44e8-d8d1-0ba561fbd3a1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics:\n",
            "Accuracy: 0.80\n",
            "Precision: 0.83\n",
            "Recall: 0.79\n",
            "F1 Score: 0.81\n"
          ]
        }
      ]
    }
  ]
}