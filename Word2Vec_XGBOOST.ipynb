{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2grT60PNTOYaxAKlFrDSi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahulappu2004/Fake-Account-Detection-in-Twitter-X/blob/main/Word2Vec_XGBOOST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec XGBOOST**"
      ],
      "metadata": {
        "id": "x5vWlfnApRrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim xgboost scikit-learn pandas\n"
      ],
      "metadata": {
        "id": "E8BTT9QPGCqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ba2705-5ca6-493a-ba1f-dd01a0566066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEtIL-05jpu6",
        "outputId": "d2a752a2-9822-4576-d519-5ebe7eeead93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries**"
      ],
      "metadata": {
        "id": "XcUYzwXleRvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import random"
      ],
      "metadata": {
        "id": "tslucbbvdUhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the dataset**"
      ],
      "metadata": {
        "id": "gYXNYq4TeGNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/twitter_label.txt'\n",
        "data = pd.read_csv(file_path, sep='\\t', header=None, names=['Tweet', 'Label'])"
      ],
      "metadata": {
        "id": "G_mdziQMdYXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the data**"
      ],
      "metadata": {
        "id": "TsBd36DceZ6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "data['Processed_Tweet'] = data['Tweet'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "VUtdR_WodbBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduce Random Typos into Tweets**\n"
      ],
      "metadata": {
        "id": "V3rFnHbyek35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_typos(tweet):\n",
        "    noisy_tweet = []\n",
        "    for word in tweet:\n",
        "        if random.random() < 0.1:\n",
        "            noisy_word = list(word)\n",
        "            if len(noisy_word) > 1:\n",
        "                i = random.randint(0, len(noisy_word) - 1)\n",
        "                noisy_word[i] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "            noisy_tweet.append(''.join(noisy_word))\n",
        "        else:\n",
        "            noisy_tweet.append(word)\n",
        "    return noisy_tweet"
      ],
      "metadata": {
        "id": "AtMZSD1ldeUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flip_label(label):\n",
        "    if random.random() < 0.05:  # 5% chance to flip the label\n",
        "        return 'Human' if label == 'Bot' else 'Bot'\n",
        "    return label\n",
        "\n",
        "data['Processed_Tweet'] = data['Processed_Tweet'].apply(add_typos)\n",
        "data['Label'] = data['Label'].apply(flip_label)"
      ],
      "metadata": {
        "id": "3nfgAFYQdlaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Word2Vec model**"
      ],
      "metadata": {
        "id": "_fy2QeHEgi16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=data['Processed_Tweet'], vector_size=100, window=5, min_count=1, workers=4, seed=42)"
      ],
      "metadata": {
        "id": "r1UQpWY4dpss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Tweet Embeddings**"
      ],
      "metadata": {
        "id": "7PKffb2Ggpnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_to_vec(tweet):\n",
        "    vectors = [w2v_model.wv[word] for word in tweet if word in w2v_model.wv]\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "\n",
        "data['Vector'] = data['Processed_Tweet'].apply(tweet_to_vec)"
      ],
      "metadata": {
        "id": "WCZ-ZHi5dsyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting 'Data Label' and 'Transformed Vector'**"
      ],
      "metadata": {
        "id": "FYQh7njRg4aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.vstack(data['Vector'])\n",
        "y = LabelEncoder().fit_transform(data['Label'])"
      ],
      "metadata": {
        "id": "7vH2PQU1dxGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "hcI9yTzAd3fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting and building XGboost algorithm**"
      ],
      "metadata": {
        "id": "2-tLcGT7hZIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Convert data to DMatrix\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Define parameters\n",
        "params = {\n",
        "    'objective': 'binary:logistic',  # Binary classification\n",
        "    'eval_metric': 'logloss',       # Evaluation metric\n",
        "    'eta': 0.1,                     # Learning rate\n",
        "    'max_depth': 4,                 # Tree depth\n",
        "    'subsample': 0.8,               # Row sampling\n",
        "    'colsample_bytree': 0.8,        # Feature sampling\n",
        "    'lambda': 1.0,                  # L2 regularization\n",
        "    'random_state': 42              # For reproducibility\n",
        "}\n",
        "\n",
        "# Specify evaluation sets\n",
        "evals = [(dtrain, 'train'), (dtest, 'test')]\n",
        "\n",
        "# Train with early stopping\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=100,           # Maximum number of boosting rounds\n",
        "    evals=evals,                   # Evaluation sets\n",
        "    early_stopping_rounds=10,      # Stop if no improvement\n",
        "    verbose_eval=False             # Suppress detailed output\n",
        ")"
      ],
      "metadata": {
        "id": "3Dc3P0t8io5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (bst.predict(dtest) > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "tNCZWRBGd_G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of performance of the model**"
      ],
      "metadata": {
        "id": "AyvEe_I1hrQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCtX1Y19eDag",
        "outputId": "41067594-5b77-4152-c906-aa126dfcf072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.91\n",
            "Precision: 0.91\n",
            "Recall: 0.91\n",
            "F1 Score: 0.91\n"
          ]
        }
      ]
    }
  ]
}